{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# Kết nối database SQLite\n",
    "DB_PATH = \"C:/Users/hoang/Data_Train_2025/database_labeled_all.db\"\n",
    "TABLE_NAME = \"one_hot_encoded_data\"\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_RATIO = 0.8 \n",
    "\n",
    "# conn = sqlite3.connect(DB_PATH)\n",
    "# cursor = conn.cursor()\n",
    "# query = \"select * from  limit 5;\"\n",
    "\n",
    "# df1= pd.read_sql_query(query, conn)\n",
    "# print (df1)\n",
    "\n",
    "#Dùng generator\n",
    "class SQLiteDataGenerator(Sequence):\n",
    "    def __init__(self, db_path, table_name, batch_size=32, mode=\"train\"):\n",
    "        self.db_path = db_path\n",
    "        self.table_name = table_name\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # Lấy số lượng mẫu và chia train/test\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {self.table_name}\")\n",
    "        self.total_samples = cursor.fetchone()[0]\n",
    "        self.train_samples = int(self.total_samples * TRAIN_RATIO)\n",
    "        conn.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return int(np.ceil(self.train_samples / self.batch_size))\n",
    "        else:\n",
    "            return int(np.ceil((self.total_samples - self.train_samples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        offset = index * self.batch_size\n",
    "        limit = self.batch_size\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            query = f\"SELECT * FROM {self.table_name} LIMIT {limit} OFFSET {offset}\"\n",
    "        else:\n",
    "            query = f\"SELECT * FROM {self.table_name} LIMIT {limit} OFFSET {self.train_samples + offset}\"\n",
    "\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        batch_df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        # Chuẩn bị dữ liệu\n",
    "        X = batch_df.drop(columns=[\"label\"]).values.reshape(-1, 46, 1)\n",
    "        y = tf.keras.utils.to_categorical(batch_df[\"label\"], num_classes=10)  # One-hot encode labels\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2451/1165872 [..............................] - ETA: 961:50:18 - loss: 0.7396 - accuracy: 0.7605"
     ]
    }
   ],
   "source": [
    "train_generator = SQLiteDataGenerator(DB_PATH, TABLE_NAME, batch_size=BATCH_SIZE, mode=\"train\")\n",
    "test_generator = SQLiteDataGenerator(DB_PATH, TABLE_NAME, batch_size=BATCH_SIZE, mode=\"test\")\n",
    "\n",
    "from tensorflow import keras\n",
    "# Định nghĩa mô hình CNN\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv1D(filters=32, kernel_size=3, padding=\"same\",activation=\"relu\",input_shape = (46, 1)),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0,5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_generator, validation_data=test_generator, epochs=10, batch_size=64)\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save(\"cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Chia dữ liệu thành tập huấn luyện và tập kiểm tra, ví dụ chia tỉ lệ 80-20\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Ở đây:\n",
    "# # - X là ma trận hoặc DataFrame chứa các đặc trưng/hồ sơ\n",
    "# # - y là mảng hoặc Series chứa nhãn hoặc biến mục tiêu\n",
    "# # - test_size là tỷ lệ dữ liệu sẽ được chia thành tập kiểm tra (ở đây là 20%)\n",
    "# # - random_state là seed để tạo số ngẫu nhiên, giúp kết quả có thể tái tạo được\n",
    "\n",
    "# # In ra kích thước của các tập dữ liệu\n",
    "# print(\"Kích thước tập huấn luyện X:\", X_train.shape)\n",
    "# print(\"Kích thước tập kiểm tra X:\", X_test.shape)\n",
    "# print(\"Kích thước tập huấn luyện y:\", y_train.shape)\n",
    "# print(\"Kích thước tập kiểm tra y:\", y_test.shape)\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# # Số lượng lớp (dựa vào y_encoded)\n",
    "# num_classes = y_encoded.shape[1]  \n",
    "\n",
    "# # Xây dựng mô hình CNN cho dữ liệu số\n",
    "# model = Sequential([\n",
    "#     Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),  \n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Conv1D(64, kernel_size=3, activation='relu'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='categorical_crossentropy',  \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Hiển thị mô hình\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "y_pred= np.argmax(pred, axis = 1)\n",
    "# y_pred = np.array(y_pred)\n",
    "\n",
    "# num_classes = len(label_mapping)\n",
    "# y_pred = to_categorical(y_pred, num_classes)\n",
    "\n",
    "print(y_pred)\n",
    "y_test_to_argmax = np.argmax(y_test, axis=1)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "confusion_matrix(y_test_to_argmax, y_pred)\n",
    "\n",
    "\n",
    "accuracy =accuracy_score(y_test_to_argmax, y_pred)*100\n",
    "print(accuracy)\n",
    "\n",
    "recall = recall_score(y_test_to_argmax, y_pred , average=\"weighted\")\n",
    "precision = precision_score(y_test_to_argmax, y_pred , average=\"weighted\")\n",
    "f1 = f1_score(y_test_to_argmax, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
